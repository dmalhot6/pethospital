name: Deploy Infrastructure and Application

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy (dev or prod)'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod

env:
  AWS_REGION: us-west-2
  TERRAFORM_VERSION: 1.5.7
  PROJECT_PREFIX: pet-hospital

permissions:
  id-token: write
  contents: read

jobs:
  setup_state_storage:
    name: Setup Terraform State Storage
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set environment
        id: set-env
        run: |
          if [ "${{ github.event.inputs.environment }}" != "" ]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            echo "environment=dev" >> $GITHUB_OUTPUT
          fi

      - name: Setup Terraform State Storage
        run: |
          # Check if S3 bucket already exists
          if aws s3api head-bucket --bucket ${{ env.PROJECT_PREFIX }}-terraform-state 2>/dev/null; then
            echo "Terraform state storage already exists. Skipping creation."
          else
            echo "Creating Terraform state storage..."
            ./infrastructure/setup-state-storage.sh --prefix ${{ env.PROJECT_PREFIX }} --region ${{ env.AWS_REGION }}
          fi

  terraform:
    name: Terraform
    needs: setup_state_storage
    runs-on: ubuntu-latest
    outputs:
      cluster_name: ${{ steps.terraform-outputs.outputs.cluster_name }}
      application_url: ${{ steps.terraform-outputs.outputs.application_url }}
      aws_account_id: ${{ steps.get-aws-account.outputs.aws_account_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Terraform Init
        working-directory: ./infrastructure
        run: terraform init

      - name: Terraform Plan
        working-directory: ./infrastructure
        run: |
          if [ "${{ needs.setup_state_storage.outputs.environment }}" == "prod" ]; then
            terraform plan -var-file=environments/prod.tfvars -out=tfplan
          else
            terraform plan -var-file=environments/terraform.tfvars -out=tfplan
          fi

      - name: Terraform Apply
        working-directory: ./infrastructure
        run: terraform apply -auto-approve tfplan

      - name: Get Terraform Outputs
        working-directory: ./infrastructure
        run: |
          # Get outputs with all the debug info
          FULL_CLUSTER_NAME=$(terraform output -raw cluster_name)
          FULL_APPLICATION_URL=$(terraform output -raw application_url)
          
          echo "Full Cluster name output: '$FULL_CLUSTER_NAME'"
          echo "Full Application URL output: '$FULL_APPLICATION_URL'"
          
          # Extract just the stdout part using grep and sed
          CLUSTER_NAME=$(echo "$FULL_CLUSTER_NAME" | grep -o "stdout: .*" | sed 's/stdout: //')
          APPLICATION_URL=$(echo "$FULL_APPLICATION_URL" | grep -o "stdout: .*" | sed 's/stdout: //')
          
          # If grep didn't find the pattern, try a simpler approach
          if [ -z "$CLUSTER_NAME" ]; then
            # Just take the first line and remove any non-printable characters
            CLUSTER_NAME=$(echo "$FULL_CLUSTER_NAME" | head -n 1 | tr -cd '[:print:]')
          fi
          
          if [ -z "$APPLICATION_URL" ]; then
            # Just take the first line and remove any non-printable characters
            APPLICATION_URL=$(echo "$FULL_APPLICATION_URL" | head -n 1 | tr -cd '[:print:]')
          fi
          
          echo "Extracted Cluster name: '$CLUSTER_NAME'"
          echo "Extracted Application URL: '$APPLICATION_URL'"
          
          # Write to files and then to GITHUB_ENV to avoid any issues
          echo "CLUSTER_NAME=${CLUSTER_NAME}" > cluster_env.txt
          echo "APPLICATION_URL=${APPLICATION_URL}" > app_url_env.txt
          
          cat cluster_env.txt >> $GITHUB_ENV
          cat app_url_env.txt >> $GITHUB_ENV

      - name: Get AWS Account ID
        id: get-aws-account
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "AWS Account ID: $AWS_ACCOUNT_ID"
          echo "aws_account_id=$AWS_ACCOUNT_ID" >> $GITHUB_OUTPUT

      - name: Set Job Outputs
        id: terraform-outputs
        run: |
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
          echo "application_url=$APPLICATION_URL" >> $GITHUB_OUTPUT

  build_and_push:
    name: Build and Push Docker Images
    needs: [terraform]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        service: [pet-service, hospital-service, doctor-service, billing-service, insurance-service, visit-service, vet-service, frontend]
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Construct ECR URL
        id: construct-ecr-url
        run: |
          SERVICE="${{ matrix.service }}"
          AWS_ACCOUNT_ID="${{ needs.terraform.outputs.aws_account_id }}"
          
          echo "Service: $SERVICE"
          echo "AWS Account ID from output: $AWS_ACCOUNT_ID"
          
          # If AWS_ACCOUNT_ID is empty, get it directly
          if [ -z "$AWS_ACCOUNT_ID" ]; then
            echo "AWS Account ID is empty, getting it directly"
            AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
            echo "AWS Account ID from direct call: $AWS_ACCOUNT_ID"
          fi
          
          # Ensure AWS_ACCOUNT_ID is not empty
          if [ -z "$AWS_ACCOUNT_ID" ]; then
            echo "Error: AWS Account ID is still empty"
            exit 1
          fi
          
          ECR_URL="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.PROJECT_PREFIX }}-${SERVICE}"
          
          echo "ECR URL: $ECR_URL"
          echo "ecr_url=$ECR_URL" >> $GITHUB_OUTPUT

      - name: Check if service directory exists
        id: check-dir
        run: |
          SERVICE="${{ matrix.service }}"
          DIR_PATH="${{ matrix.service == 'frontend' && './frontend' || format('./backend/{0}', matrix.service) }}"
          
          if [ -d "$DIR_PATH" ]; then
            echo "Directory $DIR_PATH exists"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "Directory $DIR_PATH does not exist, skipping"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      # Fix frontend nginx.conf if this is the frontend service
      - name: Fix frontend nginx.conf
        if: matrix.service == 'frontend' && steps.check-dir.outputs.exists == 'true'
        run: |
          echo "Fixing frontend nginx.conf to handle missing services..."
          
          # Create a completely new nginx.conf file with all the fixes
          cat > ./frontend/nginx.conf << 'EOF'
          server {
              listen 80;
              server_name localhost;
              root /usr/share/nginx/html;
              index index.html;

              location / {
                  try_files $uri $uri/ /index.html;
              }

              location /api/pets {
                  proxy_pass http://pet-service:3000/pets;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              location /api/hospitals {
                  proxy_pass http://hospital-service:3000/hospitals;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              location /api/doctors {
                  proxy_pass http://doctor-service:3000/doctors;
                  proxy_http_version 1.1;
                  proxy_set_header Upgrade $http_upgrade;
                  proxy_set_header Connection 'upgrade';
                  proxy_set_header Host $host;
                  proxy_cache_bypass $http_upgrade;
              }

              # Billing service not yet implemented
              location /api/billing {
                  return 501 '{"error": "Billing service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Insurance service not yet implemented
              location /api/insurance {
                  return 501 '{"error": "Insurance service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Visit service not yet implemented
              location /api/visits {
                  return 501 '{"error": "Visit service not yet implemented"}';
                  add_header Content-Type application/json;
              }

              # Vet service not yet implemented
              location /api/vets {
                  return 501 '{"error": "Vet service not yet implemented"}';
                  add_header Content-Type application/json;
              }
          }
          EOF
          
          echo "Created new nginx.conf:"
          cat ./frontend/nginx.conf

      - name: Build and push Docker image
        if: steps.check-dir.outputs.exists == 'true'
        uses: docker/build-push-action@v4
        with:
          context: ${{ matrix.service == 'frontend' && './frontend' || format('./backend/{0}', matrix.service) }}
          push: true
          tags: ${{ steps.construct-ecr-url.outputs.ecr_url }}:latest,${{ steps.construct-ecr-url.outputs.ecr_url }}:${{ github.sha }}
          build-args: |
            AWS_REGION=${{ env.AWS_REGION }}

  deploy_to_eks:
    name: Deploy to EKS
    needs: [setup_state_storage, terraform, build_and_push]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ needs.terraform.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      - name: Install ArgoCD CLI
        run: |
          curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
          sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
          rm argocd-linux-amd64

      - name: Wait for ArgoCD to be ready
        run: |
          echo "Waiting for ArgoCD server to be ready..."
          kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd

      - name: Update AWS Load Balancer Controller IAM Policy
        run: |
          # Update the IAM policy for the AWS Load Balancer Controller
          echo "Updating AWS Load Balancer Controller IAM Policy..."
          
          # Create the policy document
          cat > alb-policy.json << 'EOF'
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Action": [
                  "elasticloadbalancing:DescribeLoadBalancers",
                  "elasticloadbalancing:DescribeTargetGroups",
                  "elasticloadbalancing:DescribeTargetHealth",
                  "elasticloadbalancing:DescribeListeners",
                  "elasticloadbalancing:DescribeRules",
                  "elasticloadbalancing:DescribeListenerCertificates",
                  "elasticloadbalancing:DescribeTags",
                  "elasticloadbalancing:DescribeLoadBalancerAttributes",
                  "elasticloadbalancing:DescribeTargetGroupAttributes",
                  "elasticloadbalancing:DescribeListenerAttributes",
                  "elasticloadbalancing:ModifyLoadBalancerAttributes",
                  "elasticloadbalancing:ModifyTargetGroupAttributes",
                  "elasticloadbalancing:ModifyListener",
                  "elasticloadbalancing:ModifyRule",
                  "elasticloadbalancing:SetSubnets",
                  "elasticloadbalancing:SetSecurityGroups",
                  "elasticloadbalancing:SetIpAddressType",
                  "elasticloadbalancing:CreateLoadBalancer",
                  "elasticloadbalancing:CreateTargetGroup",
                  "elasticloadbalancing:CreateListener",
                  "elasticloadbalancing:CreateRule",
                  "elasticloadbalancing:DeleteLoadBalancer",
                  "elasticloadbalancing:DeleteTargetGroup",
                  "elasticloadbalancing:DeleteListener",
                  "elasticloadbalancing:DeleteRule",
                  "elasticloadbalancing:RegisterTargets",
                  "elasticloadbalancing:DeregisterTargets",
                  "elasticloadbalancing:AddTags",
                  "elasticloadbalancing:RemoveTags",
                  "elasticloadbalancing:AddListenerCertificates",
                  "elasticloadbalancing:RemoveListenerCertificates",
                  "elasticloadbalancing:SetWebAcl",
                  "ec2:DescribeVpcs",
                  "ec2:DescribeSubnets",
                  "ec2:DescribeSecurityGroups",
                  "ec2:DescribeInstances",
                  "ec2:DescribeInternetGateways",
                  "ec2:DescribeNatGateways",
                  "ec2:DescribeRouteTables",
                  "ec2:DescribeNetworkInterfaces",
                  "ec2:DescribeAvailabilityZones",
                  "ec2:CreateSecurityGroup",
                  "ec2:CreateTags",
                  "ec2:DeleteSecurityGroup",
                  "ec2:AuthorizeSecurityGroupIngress",
                  "ec2:RevokeSecurityGroupIngress",
                  "ec2:DeleteTags",
                  "acm:ListCertificates",
                  "acm:DescribeCertificate",
                  "iam:CreateServiceLinkedRole",
                  "wafv2:GetWebACL",
                  "wafv2:GetWebACLForResource",
                  "wafv2:AssociateWebACL",
                  "wafv2:DisassociateWebACL",
                  "shield:GetSubscriptionState",
                  "shield:DescribeProtection",
                  "shield:CreateProtection",
                  "shield:DeleteProtection",
                  "cognito-idp:DescribeUserPoolClient"
                ],
                "Resource": "*"
              }
            ]
          }
          EOF
          
          # Get the policy ARN
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='pet-hospital-eks-cluster-AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)
          
          if [ -n "$POLICY_ARN" ]; then
            echo "IAM policy exists, checking if update is needed..."
            
            # Get the default version ID
            DEFAULT_VERSION=$(aws iam get-policy --policy-arn $POLICY_ARN --query 'Policy.DefaultVersionId' --output text)
            
            # Get the current policy document
            CURRENT_POLICY=$(aws iam get-policy-version --policy-arn $POLICY_ARN --version-id $DEFAULT_VERSION --query 'PolicyVersion.Document' --output json)
            
            # Compare with our policy document (normalize both by removing whitespace)
            CURRENT_NORMALIZED=$(echo "$CURRENT_POLICY" | jq -c .)
            NEW_NORMALIZED=$(cat alb-policy.json | jq -c .)
            
            if [ "$CURRENT_NORMALIZED" != "$NEW_NORMALIZED" ]; then
              echo "Policy needs to be updated..."
              
              # List all non-default policy versions
              echo "Listing policy versions..."
              aws iam list-policy-versions --policy-arn $POLICY_ARN
              
              # Get the number of versions
              VERSION_COUNT=$(aws iam list-policy-versions --policy-arn $POLICY_ARN --query 'length(Versions)' --output text)
              echo "Current version count: $VERSION_COUNT"
              
              # If we have 5 versions, we need to delete one
              if [ "$VERSION_COUNT" -ge 5 ]; then
                # Get the oldest non-default version ID
                OLDEST_VERSION=$(aws iam list-policy-versions --policy-arn $POLICY_ARN --query 'Versions[?IsDefaultVersion==`false`].VersionId' --output text | tr '\t' '\n' | head -1)
                echo "Deleting oldest policy version: $OLDEST_VERSION"
                aws iam delete-policy-version --policy-arn $POLICY_ARN --version-id $OLDEST_VERSION
              fi
              
              # Create new policy version
              echo "Creating new policy version..."
              aws iam create-policy-version --policy-arn $POLICY_ARN --policy-document file://alb-policy.json --set-as-default
              echo "IAM policy updated successfully"
            else
              echo "IAM policy is already up to date, no changes needed"
            fi
          else
            echo "IAM policy not found, creating new policy..."
            aws iam create-policy --policy-name pet-hospital-eks-cluster-AWSLoadBalancerControllerIAMPolicy --policy-document file://alb-policy.json
            echo "IAM policy created successfully"
          fi
          
      - name: Increase IP allocation for AWS CNI
        run: |
          # Increase the IP address allocation for the AWS CNI plugin
          echo "Increasing IP address allocation for AWS CNI..."
          kubectl set env daemonset aws-node -n kube-system WARM_IP_TARGET=5
          kubectl set env daemonset aws-node -n kube-system MINIMUM_IP_TARGET=10
          
          # Wait for the changes to take effect
          echo "Waiting for AWS CNI changes to take effect..."
          sleep 30

      - name: Create namespace and ECR secret
        run: |
          # Create namespace
          echo "Creating namespace: pethospital-dev"
          kubectl create namespace pethospital-dev --dry-run=client -o yaml | kubectl apply -f -
          kubectl label namespace pethospital-dev --overwrite argocd.argoproj.io/managed-by=argocd
          
          # Create ECR registry secret - fixed format
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_TOKEN=$(aws ecr get-login-password --region ${AWS_REGION})
          
          # Create a proper Docker config JSON with explicit base64 encoding
          DOCKER_CONFIG="{\"auths\":{\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com\":{\"auth\":\"$(echo -n "AWS:${ECR_TOKEN}" | base64 -w 0)\"}}}"
          
          # Create the secret directly without base64 encoding the entire JSON
          kubectl create secret docker-registry ecr-registry-secret \
            --namespace=pethospital-dev \
            --docker-server=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com \
            --docker-username=AWS \
            --docker-password=${ECR_TOKEN} \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Verify the secret was created correctly
          echo "Verifying ECR registry secret..."
          kubectl get secret ecr-registry-secret -n pethospital-dev
          
          # Create a ConfigMap to store the ECR repository URL for use in deployments
          echo "Creating ConfigMap with ECR repository URL..."
          kubectl create configmap ecr-config \
            --namespace=pethospital-dev \
            --from-literal=repository_url=${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Fix Kubernetes manifests
        run: |
          echo "Fixing Kubernetes manifests to use proper ECR repository URLs..."
          
          # Get AWS account ID
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE_URL="${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          
          # Update frontend.yaml to use the correct ECR URL
          sed -i "s|\${ECR_REPOSITORY_URL}/frontend:latest|${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:latest|g" ./k8s/base/frontend.yaml
          sed -i "s|image: .*dkr.ecr.*.amazonaws.com/pet-hospital-frontend:latest|image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:latest|g" ./k8s/base/frontend.yaml
          
          # Check if other service manifests exist and update them
          for service in pet-service hospital-service doctor-service; do
            if [ -f "./k8s/base/${service}.yaml" ]; then
              echo "Updating ${service}.yaml..."
              sed -i "s|\${ECR_REPOSITORY_URL}/${service}:latest|${ECR_BASE_URL}/${PROJECT_PREFIX}-${service}:latest|g" ./k8s/base/${service}.yaml
              sed -i "s|image: .*dkr.ecr.*.amazonaws.com/pet-hospital-${service}:latest|image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-${service}:latest|g" ./k8s/base/${service}.yaml
            fi
          done
          
          # Update deployment patches to include explicit image references
          echo "Updating deployment patches..."
          cat > ./k8s/overlays/dev/patches/deployment-patches.yaml << EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: pet-service
          spec:
            template:
              spec:
                containers:
                  - name: pet-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-pet-service:latest
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: hospital-service
          spec:
            template:
              spec:
                containers:
                  - name: hospital-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-hospital-service:latest
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: doctor-service
          spec:
            template:
              spec:
                containers:
                  - name: doctor-service
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: AWS_REGION
                        value: "us-west-2"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-doctor-service:latest
          ---
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: frontend
          spec:
            template:
              spec:
                containers:
                  - name: frontend
                    env:
                      - name: ENVIRONMENT
                        value: "dev"
                      - name: API_URL
                        value: "http://frontend-ingress"
                    image: ${ECR_BASE_URL}/${PROJECT_PREFIX}-frontend:latest
          EOF
          
          echo "Updated deployment patches:"
          cat ./k8s/overlays/dev/patches/deployment-patches.yaml
          
          # Update ECR secret with proper format
          echo "Updating ECR secret..."
          cat > ./k8s/overlays/dev/ecr-secret.yaml << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: ecr-registry-secret
            namespace: pethospital-dev
          type: kubernetes.io/dockerconfigjson
          data:
            # This is a placeholder that will be replaced by the actual credentials in the GitHub workflow
            .dockerconfigjson: eyJhdXRocyI6eyIke0FXU19BQ0NPVU5UX0lEfS5ka3IuZWNyLiR7QVdTX1JFR0lPTn0uYW1hem9uYXdzLmNvbSI6eyJhdXRoIjoiUVZkVE9rVkRVaUJRUVZOVFYwOVNSQT09In19fQ==
          EOF
          
          echo "Updated ECR secret:"
          cat ./k8s/overlays/dev/ecr-secret.yaml

      - name: Deploy with ArgoCD
        run: |
          # Create ArgoCD application manifest
          cat > argocd-application.yaml << EOF
          apiVersion: argoproj.io/v1alpha1
          kind: Application
          metadata:
            name: pethospital
            namespace: argocd
          spec:
            project: default
            source:
              repoURL: https://github.com/${{ github.repository }}.git
              targetRevision: $GITHUB_SHA
              path: k8s/overlays/${{ needs.setup_state_storage.outputs.environment }}
            destination:
              server: https://kubernetes.default.svc
              namespace: pethospital-dev
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
                - Validate=true
                - PrunePropagationPolicy=foreground
                - PruneLast=true
          EOF
          
          # Apply the application manifest
          kubectl apply -f argocd-application.yaml
          
          # Wait for application to be created
          echo "Waiting for application to be created..."
          sleep 10
          
          # Check application status
          echo "Application status:"
          kubectl get application pethospital -n argocd
      - name: Delete and recreate pods
        run: |
          echo "Deleting all pods in pethospital-dev namespace to force recreation with correct image names..."
          kubectl delete pods --all -n pethospital-dev
          
          echo "Waiting for pods to be recreated..."
          sleep 30
          
          echo "Checking pod status after recreation..."
          kubectl get pods -n pethospital-dev

      - name: Debug Pod Status
        run: |
          echo "Checking pod status in pethospital-dev namespace..."
          kubectl get pods -n pethospital-dev
          
          echo "Checking pod details..."
          for pod in $(kubectl get pods -n pethospital-dev -o jsonpath='{.items[*].metadata.name}'); do
            echo "==== Details for pod: $pod ===="
            kubectl describe pod $pod -n pethospital-dev
          done
          
          echo "Checking pod logs..."
          for pod in $(kubectl get pods -n pethospital-dev -o jsonpath='{.items[*].metadata.name}'); do
            echo "==== Logs for pod: $pod ===="
            kubectl logs $pod -n pethospital-dev --tail=50 || echo "Failed to get logs"
          done
          
          echo "Checking events..."
          kubectl get events -n pethospital-dev --sort-by='.lastTimestamp'
          
          echo "Checking ingress status..."
          kubectl get ingress -n pethospital-dev -o wide
          
          echo "Checking services..."
          kubectl get services -n pethospital-dev -o wide

      - name: Output Application URL
        run: |
          echo "Application URL: ${{ needs.terraform.outputs.application_url }}"
          echo "APPLICATION_URL=${{ needs.terraform.outputs.application_url }}" >> $GITHUB_ENV

  notify:
    name: Notify Deployment Status
    needs: [deploy_to_eks, terraform]
    runs-on: ubuntu-latest
    steps:
      - name: Deployment Success
        run: |
          echo "Deployment completed successfully!"
          echo "Application URL: ${{ needs.terraform.outputs.application_url }}"
